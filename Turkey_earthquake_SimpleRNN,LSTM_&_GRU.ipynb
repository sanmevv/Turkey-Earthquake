{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "135t1_o_wlGxlKRojXLoRGb-g_BlCGz_n",
      "authorship_tag": "ABX9TyMCBb2b3MRx0FEunVU2UIWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanmevv/Turkey-Syria-Earthquake-Disaster-management-/blob/main/Turkey_earthquake_SimpleRNN%2CLSTM_%26_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90OFm-pGT55a",
        "outputId": "ed7cf635-cf8c-4d04-9467-863f36bb972a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: keras 2.11.0\n",
            "Uninstalling keras-2.11.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.9/dist-packages/keras-2.11.0.dist-info/*\n",
            "    /usr/local/lib/python3.9/dist-packages/keras/*\n",
            "Proceed (Y/n)? \u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 167, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/commands/uninstall.py\", line 97, in run\n",
            "    uninstall_pathset = req.uninstall(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/req/req_install.py\", line 638, in uninstall\n",
            "    uninstalled_pathset.remove(auto_confirm, verbose)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/req/req_uninstall.py\", line 363, in remove\n",
            "    if auto_confirm or self._allowed_to_proceed(verbose):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/req/req_uninstall.py\", line 403, in _allowed_to_proceed\n",
            "    return ask(\"Proceed (Y/n)? \", (\"y\", \"n\", \"\")) != \"n\"\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/utils/misc.py\", line 186, in ask\n",
            "    response = input(message)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 221, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/cli/base_command.py\", line 205, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1434, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1589, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1599, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1661, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 952, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.9/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1187, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 1083, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 927, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pip/_internal/utils/logging.py\", line 110, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 671, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.9/logging/__init__.py\", line 621, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.9/traceback.py\", line 103, in print_exception\n",
            "    for line in TracebackException(\n",
            "  File \"/usr/lib/python3.9/traceback.py\", line 517, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.9/traceback.py\", line 366, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.9/traceback.py\", line 288, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.9/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.9/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.9/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.9/tokenize.py\", line 394, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.9/tokenize.py\", line 363, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.9/tokenize.py\", line 321, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip uninstall keras\n",
        "!pip install keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5r7vPzYTij3",
        "outputId": "9681951f-834c-44c7-a3d3-dfc87442750b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade keras\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME0sCIUiUKAx",
        "outputId": "730879f2-c87e-410e-8fd3-2f577b4ad108"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall keras\n",
        "!pip install tensorflow\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jw0rtd08TmYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "metadata": {
        "id": "RsdKqYf_TpXS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras"
      ],
      "metadata": {
        "id": "gGxYO6TcUr2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfrxTRqUSO3P"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.pyplot import xticks\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import f1_score\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "!pip install keras\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,Embedding,Activation,Dropout\n",
        "from keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM\n",
        "from keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load train and test datasets\n",
        "train= pd.read_csv('/content/drive/MyDrive/train_data.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/test_data.csv')"
      ],
      "metadata": {
        "id": "A11T74IpSY0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the no. of rows and columns in the dataset\n",
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "szq_q_rwVkYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "zh7gGlyyVoqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train\n"
      ],
      "metadata": {
        "id": "b3qxt8KSV47W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.drop('Index', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "HNHGEFrLVsDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "VbC6Hma1WQ6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.isnull().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "id": "XZRFwy7QWR3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the keywords that indicate a tweet is disaster-related\n",
        "\n",
        "disaster_keywords = ['earthquake', 'Turkey', 'Syria', 'magnitude', 'tremor', 'seismic', 'Richter scale', 'epicenter', 'aftershock', 'disaster', 'rescue', 'relief', 'casualties', 'injured', 'damages', 'buildings', 'infrastructure', 'natural disaster', 'emergency', 'humanitarian']\n",
        "# Create a new column called \"Disaster-related\" and set its value to True if any of the disaster keywords appear in the tweet, otherwise set it to False\n",
        "train['Target'] = train['Tweet'].apply(lambda x: any(keyword in x.lower() for keyword in disaster_keywords))\n",
        "\n",
        "# View the updated dataframe\n",
        "train.head()"
      ],
      "metadata": {
        "id": "GPjbh5GiWrB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=train.Target)"
      ],
      "metadata": {
        "id": "wD63K3u8WYJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "3eLg8q_OXBo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lets save stopwords in a variable\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop = list(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "gDTueEhlWclH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save list of punctuation/special characters in a variable\n",
        "punctuation = list(string.punctuation)\n"
      ],
      "metadata": {
        "id": "72_ohFtlXIk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an object to convert the words to its lemma form\n",
        "lemma = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "x4lBIauiXRO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets make a combine list of stopwords and punctuations\n",
        "sw_pun = stop + punctuation"
      ],
      "metadata": {
        "id": "KLaU607rXS6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to preprocess the messages\n",
        "def preprocess(tweet):\n",
        "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet) # removing urls \n",
        "    tweet = re.sub('[^\\w]',' ',tweet) # remove embedded special characters in words (for example #earthquake)         \n",
        "    tweet = re.sub('[\\d]','',tweet) # this will remove numeric characters\n",
        "    tweet = tweet.lower()\n",
        "    words = tweet.split()  \n",
        "    sentence = \"\"\n",
        "    for word in words:     \n",
        "        if word not in (sw_pun):  # removing stopwords & punctuations                \n",
        "            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n",
        "            if len(word) > 3: # we will consider words with length  greater than 3 only\n",
        "                sentence = sentence + word + ' '             \n",
        "    return(sentence)"
      ],
      "metadata": {
        "id": "nemmGaJ6XTxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply preprocessing functions on the train and test datasets\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "train['Tweet'] = train['Tweet'].apply(lambda s : preprocess(s))\n",
        "test ['Tweet'] = test ['Tweet'].apply(lambda s : preprocess(s))"
      ],
      "metadata": {
        "id": "N2EhgGjzXW7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to remove emojis\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "metadata": {
        "id": "nyNnnFcWXdBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# applying the function on the train and the test datasets\n",
        "train['Tweet'] = train['Tweet'].apply(lambda s : remove_emoji(s))\n",
        "test ['Tweet'] = test ['Tweet'].apply(lambda s : remove_emoji(s))"
      ],
      "metadata": {
        "id": "l8-YfbSbXqqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocabulary creation"
      ],
      "metadata": {
        "id": "uiSw4vvyXxIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create vocab\n",
        "from collections import Counter\n",
        "def create_vocab(train):\n",
        "    vocab = Counter()\n",
        "    for i in range(train.shape[0]):\n",
        "        vocab.update(train.Tweet[i].split())\n",
        "    return(vocab)"
      ],
      "metadata": {
        "id": "J1mPAmUjX0wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate training and testing datasets\n",
        "master=pd.concat((train,test)).reset_index(drop=True)\n",
        "\n",
        "# call vocabulary creation function on master dataset\n",
        "vocab = create_vocab(master)\n",
        "\n",
        "# lets check the no. of words in the vocabulary\n",
        "len(vocab)"
      ],
      "metadata": {
        "id": "eId3D0lyX4pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the most common 50 words in the vocabulary\n",
        "vocab.most_common(50)"
      ],
      "metadata": {
        "id": "_PClHUHSX80H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the final vocab by considering words with more than one occurence\n",
        "final_vocab = []\n",
        "min_occur = 2\n",
        "for k,v in vocab.items():\n",
        "    if v >= min_occur:\n",
        "        final_vocab.append(k)"
      ],
      "metadata": {
        "id": "bNhOgAwSYLHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the no. of the words in the final vocabulary\n",
        "vocab_size = len(final_vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "SXpbh_M6YOl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to filter the dataset, keep only words which are present in the vocab\n",
        "def filter(tweet):\n",
        "    sentence = \"\"\n",
        "    for word in tweet.split():  \n",
        "        if word in final_vocab:\n",
        "            sentence = sentence + word + ' '\n",
        "    return(sentence)"
      ],
      "metadata": {
        "id": "KnC7rc3WYROu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply filter function on the train and test datasets\n",
        "train['Tweet'] = train['Tweet'].apply(lambda s : filter(s))\n",
        "test ['Tweet'] = test ['Tweet'].apply(lambda s : filter(s))"
      ],
      "metadata": {
        "id": "haP5F6ilYUCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets take a look at the update training dataset\n",
        "train.Tweet.head()"
      ],
      "metadata": {
        "id": "3lYekR8qYZ0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "osIikWnJYmHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the different units into which you can break down text (words, characters, or n-grams) are called tokens, \n",
        "# and breaking text into such tokens is called tokenization, this can be achieved using Tokenizer in Keras\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "    # num_words = vocab_size will create a tokenizer,configured to only take into account the vocab_size(6025)\n",
        "    tokenizer = Tokenizer(num_words=vocab_size)\n",
        "    # Build th word index, Turns strings into lists of integer indices\n",
        "    tokenizer.fit_on_texts(lines) \n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "xMkXR6BvYm4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create and apply tokenizer on the training dataset\n",
        "tokenizer = create_tokenizer(train.Tweet)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "id": "ZQZ3TEeuYrCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting texts into vectors\n",
        "train_text = tokenizer.texts_to_matrix(train.Tweet, mode = 'freq')"
      ],
      "metadata": {
        "id": "cDJxzCHQYtxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Building & Evaluation"
      ],
      "metadata": {
        "id": "aCZ79OJRY641"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Neural Network"
      ],
      "metadata": {
        "id": "WSiJkTskY89o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to calculate f1 score for each epoch\n",
        "import keras.backend as K\n",
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "metadata": {
        "id": "fmXLA0O8ZCjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "def define_model(n_words):\n",
        "    # define network\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(n_words,), activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(512,activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    # compile network\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = [get_f1])\n",
        "    \n",
        "    # summarize defined model\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "hJmttE3bZTKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "e-Oj-1MsZVhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks_list = [EarlyStopping(monitor='get_f1',patience=10,),\n",
        "ModelCheckpoint(filepath='./NN.h5',monitor='val_loss',save_best_only=True)\n",
        "]"
      ],
      "metadata": {
        "id": "fc-qICq2Zcbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the model\n",
        "n_words = X_train.shape[1]\n",
        "model = define_model(n_words)"
      ],
      "metadata": {
        "id": "USHaYCqaZhaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit network\n",
        "history = model.fit(X_train,y_train,epochs=100,verbose=2,callbacks=callbacks_list,validation_split=0.2)"
      ],
      "metadata": {
        "id": "pt2vhNCeZjKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['get_f1']\n",
        "val_acc = history.history['val_get_f1']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4HWZELV4ZmFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "\n",
        "dependencies = {\n",
        "    'get_f1': get_f1\n",
        "}\n",
        "\n",
        "\n",
        "# load the model from disk\n",
        "loaded_model_NN = keras.models.load_model('./NN.h5',custom_objects=dependencies)"
      ],
      "metadata": {
        "id": "Pm-EqdI2ZnV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction on the test dataset\n",
        "#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\n",
        "y_pred = loaded_model_NN.predict_classes(X_test)"
      ],
      "metadata": {
        "id": "q5pL0_3IZqgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# important metrices\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ayH78jz6ZuCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions on the test dataset"
      ],
      "metadata": {
        "id": "7S3ew9SxZzUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_id = test.id\n",
        "test.drop([\"id\",\"location\",\"keyword\"],1,inplace = True)\n",
        "\n",
        "# apply tokenizer on the test dataset\n",
        "test_set = tokenizer.texts_to_matrix(test.text, mode = 'freq')"
      ],
      "metadata": {
        "id": "yx-RXrhLZvRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on the test dataset\n",
        "y_test_pred = loaded_model_NN.predict_classes(test_set)"
      ],
      "metadata": {
        "id": "Ux8EqxrOZ34q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets prepare for the prediction submission\n",
        "sub = pd.DataFrame()\n",
        "sub['Id'] = test_id\n",
        "sub['target'] = y_test_pred\n",
        "sub.head()"
      ],
      "metadata": {
        "id": "c9ys07lWZ7Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub.to_csv('submission_NN.csv',index=False)"
      ],
      "metadata": {
        "id": "yGshdyneZ9Wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}